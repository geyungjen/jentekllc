{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Classification with Scala ML and MLLIB\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> This dataset does not have column name, but we will give the proper columns.  \n",
    "\n",
    "    George Jen  -- Jen Tek LLC\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark._\n",
       "import org.apache.spark.SparkContext._\n",
       "import org.apache.spark.rdd._\n",
       "import org.apache.spark.util.LongAccumulator\n",
       "import org.apache.log4j._\n",
       "import scala.collection.mutable.ArrayBuffer\n",
       "import org.apache.spark.sql._\n"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.util.LongAccumulator\n",
    "import org.apache.log4j._\n",
    "import scala.collection.mutable.ArrayBuffer\n",
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@13145723\n"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@243cced9\n"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string]\n"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val df=spark.read.option(\"sep\", \"\\t\").csv(\"datasets/SMSSpamCollection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> Show 3 lines to get an idea about the dataset,  _c0 looks like as a label, c1 looks feature </font> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c0 |_c1                                                                                                                                                        |\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|ham |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|spam|Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "+----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> Note: Spam is Spam, Han is OK.  Rename Column name _c0 as status, _c1 as feature  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_renamed: org.apache.spark.sql.DataFrame = [status: string, message: string]\n"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val df_renamed = df.withColumnRenamed(\"_c0\", \"status\").withColumnRenamed(\"_c1\", \"message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|status|message                                                                                                                                                    |\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|ham   |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|ham   |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|spam  |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_renamed.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> \n",
    "\n",
    "Encode status column to numeric: ham to 1.0 and spam to 0. All our fields need to be numeric for machine to learn, also rename the column status to label\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|message                                                                                                                                                    |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |\n",
      "|1.0  |Ok lar... Joking wif u oni...                                                                                                                              |\n",
      "|0.0  |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_select: org.apache.spark.sql.DataFrame = [label: decimal(11,1), message: string]\n"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_renamed.createOrReplaceTempView(\"temp\")\n",
    "val df_select = spark.sql(\"select case status when \\\"ham\\\" then 1.0  else 0 end as label, message from temp\")\n",
    "df_select.show(3, false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> 1 is OK, 0 is Junk </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Tokenize the messages\n",
    "Tokenization is the process of taking text (such as a sentence) and breaking it into individual terms (usually words). Let’s tokenize the messages and create a list of words of each message.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|message                                                                                                                                                    |words                                                                                                                                                                                   |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |[go, until, jurong, point,, crazy.., available, only, in, bugis, n, great, world, la, e, buffet..., cine, there, got, amore, wat...]                                                    |\n",
      "|1.0  |Ok lar... Joking wif u oni...                                                                                                                              |[ok, lar..., joking, wif, u, oni...]                                                                                                                                                    |\n",
      "|0.0  |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005., text, fa, to, 87121, to, receive, entry, question(std, txt, rate)t&c's, apply, 08452810075over18's]|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_f9575cbf17b4\n",
       "wordsData: org.apache.spark.sql.DataFrame = [label: decimal(11,1), message: string ... 1 more field]\n"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer, Tokenizer}\n",
    "val tokenizer = new Tokenizer().setInputCol(\"message\").setOutputCol(\"words\")\n",
    "val wordsData = tokenizer.transform(df_select)\n",
    "wordsData.show(3, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|label|message                                                                                                                                                    |words                                                                                                                                                                                   |rawFeatures                                                                                                                                                                                               |\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0  |Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...                                            |[go, until, jurong, point,, crazy.., available, only, in, bugis, n, great, world, la, e, buffet..., cine, there, got, amore, wat...]                                                    |(13587,[8,42,52,64,83,89,134,143,407,457,753,851,1574,1838,4696,6002,7667,8288,11559,13147],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])                            |\n",
      "|1.0  |Ok lar... Joking wif u oni...                                                                                                                              |[ok, lar..., joking, wif, u, oni...]                                                                                                                                                    |(13587,[5,75,411,573,2814,4720],[1.0,1.0,1.0,1.0,1.0,1.0])                                                                                                                                                |\n",
      "|0.0  |Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's|[free, entry, in, 2, a, wkly, comp, to, win, fa, cup, final, tkts, 21st, may, 2005., text, fa, to, 87121, to, receive, entry, question(std, txt, rate)t&c's, apply, 08452810075over18's]|(13587,[0,3,8,20,55,68,82,167,247,291,416,574,593,758,991,1085,2158,2174,2510,2627,3233,4030,4383,5413],[3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "+-----+-----------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.CountVectorizer\n",
       "count: org.apache.spark.ml.feature.CountVectorizer = cntVec_fe334b054098\n",
       "model: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_fe334b054098\n",
       "featurizedData: org.apache.spark.sql.DataFrame = [label: decimal(11,1), message: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer}\n",
    "val count = new CountVectorizer().setInputCol(\"words\").setOutputCol(\"rawFeatures\")\n",
    "val model = count.fit(wordsData)\n",
    "val featurizedData = model.transform(wordsData)\n",
    "featurizedData.show(3,false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5> CountVectorizer converts a collection of text documents to vectors of token counts. \n",
    "    \n",
    "See:\n",
    "https://spark.apache.org/docs/latest/ml-features#countvectorizer\n",
    "\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.IDF\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_9c988901769a\n",
       "idfModel: org.apache.spark.ml.feature.IDFModel = idf_9c988901769a\n",
       "rescaledData: org.apache.spark.sql.DataFrame = [label: decimal(11,1), message: string ... 3 more fields]\n",
       "rescaledData_select: org.apache.spark.sql.DataFrame = [label: decimal(11,1), features: vector]\n"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{IDF}\n",
    "val idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\")\n",
    "val idfModel = idf.fit(featurizedData)\n",
    "val rescaledData = idfModel.transform(featurizedData)\n",
    "val rescaledData_select=rescaledData.select(\"label\", \"features\")  //Only needed to train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Apply Term frequency - inverse document frequency (TF-IDF)\n",
    "\n",
    "#IDF reduces the features that often appear in the corpus. When using text as a feature, this usually improves performance because the most common, and therefore less important, words are weighted down.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Randomly Split DataFrame into 80% Training (trainDF) and 20 Testing (testDF)\n",
    "    \n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "seed: Int = 0\n",
       "trainDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: decimal(11,1), features: vector]\n",
       "testDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: decimal(11,1), features: vector]\n"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val seed = 0  // random seed 0\n",
    "//val trainDF, testDF = rescaledData.randomSplit(Array(0.8,0.2),seed)\n",
    "val Array(trainDF, testDF) = rescaledData_select.randomSplit(Array(0.8,0.2),seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Try different classifiers. \n",
    "\n",
    "Logistic regression classifier\n",
    "\n",
    "Logistic regression is a common method of predicting classification responses. A special case of a generalized linear model is the probability of predicting a result. In spark.ml, logistic regression can be used to predict binary results by binomial logistic regression, or it can be used to predict multiple types of results by using multiple logistic regression. Use the family parameter to choose between these two algorithms, or leave it unset and Spark will infer the correct variable.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
       "import org.apache.spark.ml.feature.StringIndexer\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.{Pipeline, PipelineStage}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.ml.feature.StringIndexer\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import scala.collection.mutable\n",
       "maxIter: Int = 100\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_340c1dcb9166\n"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import scala.collection.mutable\n",
    "val maxIter=100\n",
    "val lr = new LogisticRegression()\n",
    "      .setFeaturesCol(\"features\")\n",
    "      .setLabelCol(\"label\")\n",
    "      .setMaxIter(maxIter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_lr: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_340c1dcb9166, numClasses = 2, numFeatures = 13587\n"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_lr = lr.fit(trainDF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction_lr: org.apache.spark.sql.DataFrame = [label: decimal(11,1), features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prediction_lr = model_lr.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_eval_aoc: org.apache.spark.ml.evaluation.BinaryClassificationEvaluator = binEval_abb7e81e9417\n",
       "res84: Double = 0.8988486413185208\n"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val my_eval_aoc = new BinaryClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setRawPredictionCol(\"prediction\").setMetricName(\"areaUnderROC\")\n",
    "\n",
    "my_eval_aoc.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "my_mc_f1: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_b4ba79f37c9d\n",
       "res85: Double = 0.9708789304513041\n"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "var my_mc_f1 = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\").setMetricName(\"f1\")\n",
    "my_mc_f1.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_mc_accu: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_276829470e67\n",
       "res86: Double = 0.9721739130434782\n"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val my_mc_accu = new MulticlassClassificationEvaluator().setPredictionCol(\"prediction\").setLabelCol(\"label\").setMetricName(\"accuracy\")\n",
    "my_mc_accu.evaluate(prediction_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  0.0|       1.0|   31|\n",
      "|  0.0|       0.0|  123|\n",
      "|  1.0|       1.0|  995|\n",
      "|  1.0|       0.0|    1|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "train_fit_lr: org.apache.spark.sql.DataFrame = [label: decimal(11,1), prediction: double]\n"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val train_fit_lr = prediction_lr.select(\"label\",\"prediction\")\n",
    "train_fit_lr.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Naive Bayes \n",
    "Naive Bayesian classifiers are a class of simple probability classifiers that apply strong (naive) independent assumptions between features based on Bayes' theorem. The spark.ml implementation currently supports polynomial naive Bayes and Bernoulli Naïve Bayes.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.NaiveBayes\n",
       "nb: org.apache.spark.ml.classification.NaiveBayes = nb_573cfc12507b\n",
       "Model_nb: org.apache.spark.ml.classification.NaiveBayesModel = NaiveBayesModel (uid=nb_573cfc12507b) with 2 classes\n"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{NaiveBayes}\n",
    "val nb = new NaiveBayes()\n",
    "val Model_nb = nb.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "|  0.0|       0.0|\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_nb: org.apache.spark.sql.DataFrame = [label: decimal(11,1), features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_nb = Model_nb.transform(testDF)\n",
    "predictions_nb.select(\"label\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res89: Double = 0.945880926302613\n"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_eval_aoc.evaluate(predictions_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res90: Double = 0.9392549536435162\n"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mc_f1.evaluate(predictions_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res91: Double = 0.9347826086956522\n"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_mc_accu.evaluate(predictions_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Now let's try Random Forest Classfication to see how it performs on the classification on the same data\n",
    "    \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.RandomForestClassifier\n"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassifier}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rf: org.apache.spark.ml.classification.RandomForestClassifier = rfc_79242216eea3\n",
       "Model_rf: org.apache.spark.ml.classification.RandomForestClassificationModel = RandomForestClassificationModel (uid=rfc_79242216eea3) with 20 trees\n"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf = new RandomForestClassifier().setFeaturesCol(\"features\").setLabelCol(\"label\").setPredictionCol(\"prediction\").setProbabilityCol(\"probability\").setRawPredictionCol(\"rawPrediction\").setMaxDepth(3)\n",
    "val Model_rf = rf.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|0.0  |1.0       |\n",
      "|0.0  |1.0       |\n",
      "|0.0  |1.0       |\n",
      "|0.0  |1.0       |\n",
      "|0.0  |1.0       |\n",
      "+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions_rf: org.apache.spark.sql.DataFrame = [label: decimal(11,1), features: vector ... 3 more fields]\n"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions_rf = Model_rf.transform(testDF)\n",
    "predictions_rf.select(\"label\", \"prediction\").show(5,false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res93: Double = 0.5032467532467533\n"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_eval_aoc.evaluate(predictions_rf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Given Area Under Curve is 0.5, we do not want to use this Random Forest Classification.  Area under Curve is between 0 to 1,\n",
    "the more close to 1 the better the classication is.\n",
    "\n",
    "We wil give up Random Forest on this classitication\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "There are 2 machine learning libraries from Apache Spark.\n",
    "\n",
    "ml and mllib.  Data for ml is in the form of DataFrame, but in mllib, data is requried in the form of RDD (Resilient Distributed Datasets), on top of that, mllib training and testing data are required to be in the format of labeled point. \n",
    "    \n",
    "In fact, it is lot harder to prepare data for machine learning in mllib than in ml.\n",
    "\n",
    "Now we switch from ml to mllib with scala.\n",
    "    \n",
    "    \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import org.apache.spark.mllib.linalg.SparseVector\n",
    "import org.apache.spark.mllib.regression.LabeledPoint\n",
    "import org.apache.spark.ml.linalg.{Vector=>MLVector, Vectors=>MLVectors}\n",
    "import org.apache.spark.mllib.linalg.{Vector=>MLLibVector, Vectors=>MLLibVectors}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDF_2: org.apache.spark.sql.DataFrame = [first: double, features: vector]\n"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDF_2=trainDF.selectExpr(\"label as first\",\"features\").selectExpr(\"cast(first as double)\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDF_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[936] at rdd at <console>:110\n"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val trainDF_rdd=trainDF_2.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "Below is to define LabeledPoint, which takes a line from RDD and returns line that complies with LabeledPoint specification.\n",
    "    \n",
    "    \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trainDF_lp: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[937] at map at <console>:110\n"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val trainDF_lp : RDD[LabeledPoint] = trainDF_rdd.map(row => LabeledPoint(row.getAs[Double](\"first\"), \n",
    "  MLLibVectors.fromML(row.getAs[org.apache.spark.ml.linalg.SparseVector](\"features\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,(13587,[0,1,2,3,6,8,13,14,17,47,48,51,72,86,90,150,154,357,538,663,1885,4133,7302],[2.3966175115908523,1.2464154437187664,5.473359676673107,1.5612885685365177,1.9565495064704403,1.970607245960672,2.3215987939063383,2.3607463835906097,5.083096366506297,3.137109870171633,6.265972305975542,3.2278848948105665,3.649313853907745,3.7508502731271682,3.7356984681065657,4.295314256041989,4.39194109173106,5.192060391843174,5.581525158604896,5.792834252272104,6.8342881271002645,7.52743530766021,7.9329004157683745]))\n",
      "(0.0,(13587,[0,1,2,3,6,13,40,58,68,83,95,99,100,131,206,212,285,338,368,528,549,1550,1587,2036,2207,10256,12569,12601],[3.5949262673862785,2.492830887437533,1.3683399191682768,1.5612885685365177,1.9565495064704403,2.3215987939063383,3.0313362167264803,3.2981714275387386,3.5634525633013525,3.822026551595063,7.74491481044391,3.8553629718626548,3.9255672305359033,4.231598441655881,4.5656045857819,4.60069590559317,5.04252865787221,5.192060391843174,5.224850214666164,5.581525158604896,5.630315322774329,6.680137447273006,6.680137447273006,6.8342881271002645,7.016609683894219,7.9329004157683745,7.9329004157683745,7.9329004157683745]))\n"
     ]
    }
   ],
   "source": [
    "trainDF_lp.take(2).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Train the SVMwithSGD model using RDD that complies with Labeled Point\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.classification.{SVMWithSGD, SVMModel}\n"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.mllib.classification.{SVMWithSGD,SVMModel}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_mllib_svm: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 13587, numClasses = 2, threshold = 0.0\n"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_mllib_svm = SVMWithSGD.train(trainDF_lp, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res95: model_mllib_svm.type = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 13587, numClasses = 2, threshold = None\n"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_mllib_svm.clearThreshold()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Evaluate SVM model using testing data, which also needs to be converted to Labeled Ponint format.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testDF_2: org.apache.spark.sql.DataFrame = [first: double, features: vector]\n"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDF_2=testDF.selectExpr(\"label as first\",\"features\").selectExpr(\"cast(first as double)\",\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testDF_rdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[1146] at rdd at <console>:111\n"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDF_rdd=testDF_2.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "testDF_lp: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[1147] at map at <console>:111\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val testDF_lp : RDD[LabeledPoint] = testDF_rdd.map(row => LabeledPoint(row.getAs[Double](\"first\"), \n",
    "  MLLibVectors.fromML(row.getAs[org.apache.spark.ml.linalg.SparseVector](\"features\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1148] at map at <console>:114\n"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute raw scores on the test set.\n",
    "val scoreAndLabels = testDF_lp.map { point =>\n",
    "        val score = model_mllib_svm.predict(point.features)\n",
    "          (score, point.label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.967069577009336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@5732aa7f\n",
       "auROC: Double = 0.967069577009336\n"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get evaluation metrics.\n",
    "import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "\n",
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val auROC = metrics.areaUnderROC()\n",
    "\n",
    "println(s\"Area under ROC = $auROC\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "\n",
    "Since at it, try LogisticRegressionWithLBFGS under mllib with the same way on SVM above.\n",
    "\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\n"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import org.apache.spark.mllib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_mllib_lr_BFGS: org.apache.spark.mllib.classification.LogisticRegressionModel = org.apache.spark.mllib.classification.LogisticRegressionModel: intercept = 0.0, numFeatures = 13587, numClasses = 2, threshold = 0.5\n"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model_mllib_lr_BFGS = new LogisticRegressionWithLBFGS().setNumClasses(2).run(trainDF_lp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1218] at map at <console>:116\n"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Compute raw scores on the test set.\n",
    "val scoreAndLabels = testDF_lp.map { point =>\n",
    "        val score = model_mllib_lr_BFGS.predict(point.features)\n",
    "          (score, point.label)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area under ROC = 0.8895777916862253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "metrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@4f22febf\n",
       "auROC: Double = 0.8895777916862253\n"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
    "val auROC = metrics.areaUnderROC()\n",
    "\n",
    "println(s\"Area under ROC = $auROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=5>\n",
    "This concludes scala ML/MLLIIB classification Excercise.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
